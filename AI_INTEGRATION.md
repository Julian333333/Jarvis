# ğŸ¤– JARVIS AI Integration - Anleitung

## âœ… Die AI-Integration ist fertig!

Die JARVIS App kann jetzt mit Ollama kommunizieren und intelligente Antworten generieren.

## ğŸš€ Schnellstart

### 1. Ollama installieren und starten

```powershell
# Ollama herunterladen von: https://ollama.ai
# Nach Installation:

# Ollama Server starten
ollama serve

# In einem neuen Terminal: Modell herunterladen
ollama pull llama2
# oder ein anderes Modell:
ollama pull mistral
ollama pull codellama
```

### 2. JARVIS starten

```powershell
.\Start-JarvisApp.ps1
```

### 3. Mit der AI chatten

- **Frage eingeben** in das groÃŸe Textfeld
- **Enter drÃ¼cken** oder auf "Send" klicken
- **Live-Streaming** der Antwort sehen
- **Shift+Enter** fÃ¼r neue Zeile ohne Senden

## ğŸ¯ Features

### âœ… Implementiert

- **Ollama-Integration** - Verbindung zu lokalem Ollama Server (localhost:11434)
- **Streaming-Antworten** - Echtzeit-Token-Generierung
- **Modell-Erkennung** - Zeigt verfÃ¼gbare Modelle an
- **Status-Anzeige** - PrÃ¼ft ob Ollama lÃ¤uft
- **Keyboard-Shortcuts** - Enter zum Senden, Shift+Enter fÃ¼r neue Zeile
- **Clear-Funktion** - Konversation zurÃ¼cksetzen
- **Fehlerbehandlung** - Zeigt verstÃ¤ndliche Fehlermeldungen

### ğŸ¨ UI-Elemente

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JARVIS AI Assistant            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                 â”‚
â”‚  [Eingabefeld]                  â”‚
â”‚  Shift+Enter fÃ¼r neue Zeile     â”‚
â”‚                                 â”‚
â”‚  [Send]  [Clear]                â”‚
â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ AI Antwort erscheint      â”‚  â”‚
â”‚  â”‚ hier in Echtzeit...       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                 â”‚
â”‚  Status: âœ… Ollama verbunden    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Code-Struktur

### AIService.cs

```csharp
// Hauptklasse fÃ¼r AI-Kommunikation
- IsOllamaRunningAsync()      // PrÃ¼ft Verbindung
- GetAvailableModelsAsync()    // Listet Modelle
- GenerateStreamingResponseAsync()  // Streaming-Chat
```

### MainWindow.xaml.cs

```csharp
// Event-Handler
- CheckOllamaStatusAsync()     // Status-Check beim Start
- SendButton_Click()           // Sendet Anfrage
- InputTextBox_KeyDown()       // Enter-Taste Handler
- ClearButton_Click()          // LÃ¶scht Konversation
```

## ğŸ”§ Konfiguration

### Modell Ã¤ndern

In `AIService.cs`:

```csharp
private const string DefaultModel = "llama2";  // Hier Ã¤ndern
```

VerfÃ¼gbare Modelle:
- `llama2` (Standard, gut fÃ¼r Chat)
- `mistral` (Schneller, kleiner)
- `codellama` (Programmier-Spezialist)
- `phi` (Sehr klein, schnell)

### Ollama-URL Ã¤ndern

```csharp
private readonly string _ollamaUrl = "http://localhost:11434";
```

## ğŸ› Fehlerbehebung

### "âš ï¸ Ollama nicht gefunden"

**Problem:** Ollama Server lÃ¤uft nicht

**LÃ¶sung:**
```powershell
ollama serve
```

### "Fehler: No connection could be made"

**Problem:** Ollama ist nicht installiert oder lÃ¤uft auf anderem Port

**LÃ¶sung:**
1. Installiere Ollama: https://ollama.ai
2. Starte: `ollama serve`
3. PrÃ¼fe Port: Sollte 11434 sein

### Modell nicht gefunden

**Problem:** Modell wurde nicht heruntergeladen

**LÃ¶sung:**
```powershell
ollama list          # Zeige installierte Modelle
ollama pull llama2   # Lade Modell herunter
```

### App reagiert nicht

**Problem:** GroÃŸes Modell braucht lange zum Antworten

**LÃ¶sung:**
- Warte ab (erste Token kÃ¶nnen 5-10 Sek dauern)
- Nutze kleineres Modell: `ollama pull phi`
- PrÃ¼fe CPU/RAM Auslastung

## ğŸ’¡ Tipps

### Bessere Antworten

**Kontext geben:**
```
Ich bin ein Python-Entwickler. ErklÃ¤re mir async/await in C#.
```

**Rolle definieren:**
```
Du bist ein Windows-Experte. Wie optimiere ich WinUI 3 Performance?
```

**Schrittweise fragen:**
```
1. Was ist MVVM?
2. Zeige mir ein Beispiel
3. Wie nutze ich es in WinUI 3?
```

### Performance optimieren

**Schnelleres Modell verwenden:**
```powershell
ollama pull phi      # Sehr schnell, 2.7B Parameter
```

**GPU beschleunigen:**
- Ollama nutzt automatisch NVIDIA GPU falls verfÃ¼gbar
- AMD/Intel: CPU-Only aber immer noch schnell

**Mehrere Modelle gleichzeitig:**
```powershell
# Terminal 1
ollama serve

# Terminal 2
ollama pull llama2 mistral phi codellama
```

## ğŸ“Š Beispiel-Konversation

```
USER: ErklÃ¤re mir WinUI 3 in einem Satz

JARVIS: WinUI 3 ist Microsofts modernes UI-Framework fÃ¼r 
Windows-Apps mit Fluent Design und nativer Performance.

USER: Zeige mir ein Hello World Beispiel

JARVIS: Hier ist ein einfaches WinUI 3 Hello World:

<Window
    x:Class="HelloWorld.MainWindow"
    xmlns="http://schemas.microsoft.com/winfx/2006/xaml/presentation">
    <StackPanel HorizontalAlignment="Center" 
                VerticalAlignment="Center">
        <TextBlock Text="Hello, World!" 
                   Style="{StaticResource TitleTextBlockStyle}"/>
    </StackPanel>
</Window>
```

## ğŸ¯ NÃ¤chste Schritte

MÃ¶gliche Erweiterungen:

- [ ] **Conversation History** - Speichere Chat-Verlauf
- [ ] **Multiple Models** - Wechsel zwischen Modellen in UI
- [ ] **Voice Input** - Spracherkennung hinzufÃ¼gen
- [ ] **System Prompts** - Vordefinierte PersÃ¶nlichkeiten
- [ ] **Export Chat** - Konversationen als Markdown speichern
- [ ] **Dark/Light Mode Toggle** - Theme-Umschaltung

---

## âœ… Status

**AI-Integration:** âœ… VollstÃ¤ndig funktionsfÃ¤hig!

**Getestet mit:**
- Ollama 0.1.x
- Models: llama2, mistral, phi
- Windows 11 + .NET 8

**Ready to use!** ğŸ‰
