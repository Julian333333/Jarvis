# Ollama Integration Anleitung

## Schritt 1: Ollama Server starten

√ñffnen Sie ein neues Terminal/PowerShell-Fenster und f√ºhren Sie aus:

```powershell
# Falls Ollama nicht im PATH ist, verwenden Sie den vollst√§ndigen Pfad:
& "C:\Users\julia\AppData\Local\Programs\Ollama\ollama.exe" serve

# Oder falls im PATH verf√ºgbar:
ollama serve
```

Dies startet den Ollama-Server auf `localhost:11434`

## Schritt 2: Modell verwenden

Ihr JARVIS ist bereits konfiguriert f√ºr:
- **Modell**: `deepseek-r1:1.5b`
- **API**: `http://localhost:11434/api/generate`

## Schritt 3: Verf√ºgbare Befehle

### AI-Modell verwalten:
- **"Verf√ºgbare Modelle"** - Zeigt alle installierten Modelle
- **"Modell [Name]"** - Wechselt zu einem anderen Modell
- **"AI Modell"** - Zeigt verf√ºgbare Modelle

### Beispiele:
```
"Verf√ºgbare Modelle"
"Modell llama2"
"Modell deepseek-r1:1.5b"
```

## Schritt 4: Testen

1. Starten Sie JARVIS: `python -m jarvis`
2. Aktivieren Sie den Gespr√§chsmodus: "üí¨ GESPR√ÑCH" Button
3. Sprechen Sie: "Hallo JARVIS, wie geht es dir?"

## Fehlerbehebung

### Fehler: "Ollama ist nicht erreichbar"
- Stellen Sie sicher, dass `ollama serve` l√§uft
- Pr√ºfen Sie, ob Port 11434 frei ist

### Fehler: "Modell nicht gefunden"
- Pr√ºfen Sie installierte Modelle: `ollama list`
- Installieren Sie Ihr Modell: `ollama pull deepseek-r1:1.5b`

### Langsame Antworten
- Das ist normal bei lokalen Modellen
- Kleinere Modelle sind schneller
- GPU-Beschleunigung verbessert Performance

## Modell-Empfehlungen

**F√ºr bessere Performance:**
- `deepseek-r1:1.5b` (Ihr aktuelles, gut f√ºr Deutsche Sprache)
- `llama3.2:1b` (Schneller, weniger Speicherverbrauch)
- `phi3:3.8b` (Guter Kompromiss)

**F√ºr bessere Qualit√§t (aber langsamer):**
- `llama3.2:3b`
- `deepseek-r1:7b` (falls Sie genug RAM haben)

## Status pr√ºfen

JARVIS zeigt beim Start automatisch:
- ‚úÖ Ollama-Verbindung
- üìã Verf√ºgbare Modelle
- ‚öôÔ∏è Aktuelles Modell